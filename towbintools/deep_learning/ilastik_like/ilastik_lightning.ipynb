{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretrained_microscopy_models as pmm\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os \n",
    "import random\n",
    "from towbintools.foundation import image_handling\n",
    "from towbintools.foundation import binary_image\n",
    "import cv2\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from time import perf_counter\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import xgboost as xgb\n",
    "\n",
    "from csbdeep.utils import normalize\n",
    "import cv2\n",
    "from towbintools.foundation import image_handling\n",
    "from towbintools.foundation import binary_image\n",
    "from time import perf_counter\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_VGG16_micronet():\n",
    "\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', weights=None)\n",
    "    url = pmm.util.get_pretrained_microscopynet_url('vgg16_bn', 'image-micronet')\n",
    "\n",
    "    # remove classifier from model \n",
    "    model.classifier = nn.Sequential(*list(model.classifier.children())[:-8])\n",
    "    # Load pretrained weights\n",
    "\n",
    "    model.load_state_dict(model_zoo.load_url(url, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()  # <- MicrosNet model for classifcation or transfer learning\n",
    "\n",
    "    # Extract up to 'block1_conv2'\n",
    "    features_seq = list(model.features)[:2]\n",
    "    new_model = nn.Sequential(*features_seq)\n",
    "\n",
    "    # Disable gradient computation (use pretrained weights)\n",
    "    for param in new_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_to_rgb_pytorch(grayscale_img):\n",
    "    img = np.expand_dims(grayscale_img, axis=0)\n",
    "    stacked_img = np.stack((img,)*3, axis=0)\n",
    "    stacked_img = np.squeeze(stacked_img)\n",
    "    return stacked_img\n",
    "\n",
    "def transform_img(img):\n",
    "    img = normalize(img,1,99.8,axis=(0,1))\n",
    "    img = image_handling.normalize_image(img, np.uint8)\n",
    "    img = grayscale_to_rgb_pytorch(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def preprocess_ground_truth_mask(ground_truth):\n",
    "\tground_truth = cv2.morphologyEx(ground_truth, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7)))\n",
    "\tground_truth = binary_fill_holes(ground_truth).astype(np.uint8)\n",
    "\tground_truth = binary_image.get_biggest_object(ground_truth).astype(np.uint8)\n",
    "\n",
    "\tkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "\tdilated_ground_truth = (cv2.morphologyEx(ground_truth, cv2.MORPH_DILATE, kernel) > 0).astype(int)\n",
    "\tbackground = (dilated_ground_truth == 0).astype(int)\n",
    "\tground_truth = (cv2.morphologyEx(ground_truth, cv2.MORPH_ERODE, kernel) > 0).astype(int)\n",
    "\n",
    "\treturn ground_truth, background\n",
    "\n",
    "def annotations_to_tensor_pytorch(feature_matrix, mask):\n",
    "\t'''Convert the user annotated labels from napari to tensors to train the classifier on.\n",
    "\tfeature_matrix dim: [x, y, nb_features]\n",
    "\tpossible mask elements: 0: not annotated, int[1,2]: class annotation\n",
    "\t'''\n",
    "\n",
    "\t# Find the indices where mask is not -1\n",
    "\tindices = torch.nonzero(mask != -1, as_tuple=True)\n",
    "\n",
    "\t# Use those indices to extract y_labels from mask\n",
    "\ty_labels = mask[indices[0], indices[1]]\n",
    "\n",
    "\t# Use those indices to extract feature vectors\n",
    "\tX_features = feature_matrix[indices[0], indices[1]]\n",
    "\n",
    "\tX_features = X_features.cpu().numpy()\n",
    "\ty_labels = y_labels.cpu().numpy()\n",
    "\n",
    "\treturn X_features, y_labels\n",
    "\n",
    "def extract_features_and_ground_truth(features, ground_truth):\n",
    "\tground_truth, background = preprocess_ground_truth_mask(ground_truth)\n",
    "\t# replace 1 in ground truth with 2\n",
    "\tground_truth[ground_truth == 1] = 1\n",
    "\t# replace everything else with -1\n",
    "\tground_truth[ground_truth == 0] = -1\n",
    "\t# replace background in ground truth with 1\n",
    "\tground_truth[background == 1] = 0    \n",
    "\n",
    "\t# Find indices where ground_truth is 0\n",
    "\tzero_indices = np.argwhere(ground_truth == 0)\n",
    "\n",
    "\t# Randomly select 99% of the zero indices\n",
    "\tnum_samples = int(0.95 * len(zero_indices))\n",
    "\trandom_indices = np.random.choice(len(zero_indices), num_samples, replace=False)\n",
    "\tselected_zero_indices = zero_indices[random_indices]\n",
    "\n",
    "\t# Set those pixels to -1\n",
    "\tground_truth[selected_zero_indices[:, 0], selected_zero_indices[:, 1]] = -1\n",
    "\n",
    "\tX_, y_ = annotations_to_tensor_pytorch(features, torch.tensor(ground_truth))\n",
    "\treturn X_, y_\n",
    "\n",
    "class IlastikLikeTrainingDataset(Dataset):\n",
    "\tdef __init__(self, images, ground_truth, transform=None):\n",
    "\t\tself.images = images\n",
    "\t\tself.ground_truth = ground_truth\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images)\n",
    "\n",
    "\tdef __getitem__(self, i):\n",
    "\t\t\n",
    "\t\timg = image_handling.read_tiff_file(self.images[i], [2]).astype(np.float32)\n",
    "\t\timg = transform_img(img)\n",
    "\t\tmask = image_handling.read_tiff_file(self.ground_truth[i])\n",
    "\n",
    "\t\treturn torch.tensor(img, dtype=torch.float32), mask\n",
    "\n",
    "class IlastikLikeSegmentationTrainer(pl.LightningModule):\n",
    "\tdef __init__(self, shapes):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = init_VGG16_micronet()\n",
    "\t\tself.shapes = shapes\n",
    "\n",
    "\n",
    "\tdef predict_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "\t\tx, y = batch\n",
    "\t\tfeatures = []\n",
    "\t\tfor shape in self.shapes:\n",
    "\t\t\trescaled_batch = F.interpolate(x, size=shape, mode='bilinear', align_corners=True)\n",
    "\t\t\tscaled_features = self.model(rescaled_batch)\n",
    "\t\t\trescaled_features = F.interpolate(scaled_features, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "\t\t\trescaled_features = rescaled_features.squeeze(0)\n",
    "\t\t\tif rescaled_features.ndim == 3:\n",
    "\t\t\t\trescaled_features = rescaled_features.unsqueeze(0)\n",
    "\t\t\tfeatures.append([rescaled_feature for rescaled_feature in rescaled_features])\n",
    "\n",
    "\t\tglobal_features = []\n",
    "\t\tfor i in range(x.shape[0]):\n",
    "\t\t\tfeature_list = [feat[i] for feat in features]\n",
    "\t\t\tconcatenated_features = torch.cat(feature_list, dim=0)\n",
    "\t\t\tglobal_features.append(concatenated_features)\n",
    "\n",
    "\t\tX, Y = [], []\n",
    "\t\tfor features, ground_truth in zip(global_features, y):\n",
    "\t\t\tfeatures = torch.transpose(features, 2, 0)\n",
    "\t\t\tfeatures = torch.transpose(features, 0, 1)\n",
    "\t\t\tground_truth = ground_truth.cpu().numpy()\n",
    "\n",
    "\t\t\tX_, Y_ = extract_features_and_ground_truth(features, ground_truth)\n",
    "\t\t\tX.append(X_)\n",
    "\t\t\tY.append(Y_)\n",
    "\n",
    "\t\treturn {'X': X, 'Y': Y}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/spsalmon/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/spsalmon/env_directory/towbintools/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /home/spsalmon/env_directory/towbintools/lib/python ...\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1baa588edf641c387053d34ad270650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_dir = \"/mnt/external.data/TowbinLab/kstojanovski/20220401_Ti2_20x_160-182-190_pumping_25C_20220401_173300_429/analysis/ch1/\"\n",
    "images = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.tiff')]\n",
    "random.seed(42)\n",
    "training_images = random.sample(images, 3)\n",
    "training_ground_truth = [f.replace('ch1/', 'ch1_il/seg_') for f in training_images]\n",
    "\n",
    "test_loader = DataLoader(IlastikLikeTrainingDataset(training_images, training_ground_truth, transform=transform_img), batch_size=1, shuffle=False, num_workers=4, pin_memory=False)\n",
    "shapes = []\n",
    "for i in [1, 2, 4, 8]:\n",
    "    shapes.append((int(2048/i),(int(2044/i))))\n",
    "model = IlastikLikeSegmentationTrainer(shapes=shapes)\n",
    "trainer = pl.Trainer()\n",
    "predictions = trainer.predict(model, test_loader)\n",
    "predictions = [{'X' : batch['X'][i], 'Y' : batch['Y'][i]} for batch in predictions for i in range(len(batch['X']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219485, 256) (219485,)\n",
      "(220149, 256) (220149,)\n",
      "(212485, 256) (212485,)\n"
     ]
    }
   ],
   "source": [
    "for p in predictions:\n",
    "    print(p['X'].shape, p['Y'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "towbintools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
